[
  {
    "objectID": "posts/20220912_testing.html",
    "href": "posts/20220912_testing.html",
    "title": "Central Limit Theorem - Misconception 1",
    "section": "",
    "text": "Example 1: Uniform Distribution\nSuppose we are given a dataset consists of numbers uniformly distributed between 0 and 1.\n\n\nCode\ndata = np.random.uniform(size=10000)\n_ = sns.histplot(data)\n\n\n\n\n\nFigure 1: the population is of uniform distribution\n\n\n\n\nWe can randomly sample 100 data points from the population and calculate the sample mean.\n\nsample = np.random.choice(data, 100)\nsample.mean()\n\n0.5453087847471367\n\n\nThe sample mean is a random variable because of the random sampling process.\n\nfor _ in range(5):\n    sample = np.random.choice(data, 100)\n    print(sample.mean())\n\n0.4780188476724967\n0.4884357038347728\n0.49375289478957385\n0.46197942889944293\n0.4611685890607802\n\n\nIf we run the sampling process 10000 times, and plot the distribution of the sample means, we can see that it is indeed approximately of normal distribution.\n\n\nCode\nsampling_means = []\nfor _ in range(10000):\n    sample = np.random.choice(data, 100)\n    mean = sample.mean()\n    sampling_means.append(mean)\n_ = sns.histplot(sampling_means)\n\n\n\n\n\nFigure 2: the sample mean is approximately of normal distribution\n\n\n\n\nAs we increase the sample size, we can see the sample mean is converging to the expected value: 0.5, that is, the sampling error decresases.\n\n\nCode\nsampling_means = []\nfor n in [100, 300, 500]:\n    for _ in range(10000):\n        sample = np.random.choice(data, n)\n        mean = sample.mean()\n        sampling_means.append({\"n\": str(n), \"data\": mean})\n\ndf = pd.DataFrame(sampling_means)\n_ = sns.displot(df, x=\"data\", kind=\"kde\", hue=\"n\")\n\n\n\n\n\nFigure 3: the sample mean distribution for different sample sizes\n\n\n\n\n\n\nExample 2: Highly Skewed Distribution\nBut the power of the Central Limit Theorem is that it can be applied to any distribution with a finite mean and standard deviation. So even if we are sampling from a highly skewed distribution, the sample mean itself is going to be of normal distribution, and hence balanced.\n\n\nCode\ndata = np.random.beta(5, 2, size=10000)\n_ = sns.histplot(data)\n\n\n\n\n\nFigure 4: the population is skewed to the right\n\n\n\n\n\n\nCode\nsampling_means = []\nfor _ in range(10000):\n    sample = np.random.choice(data, 200)\n    mean = sample.mean()\n    sampling_means.append(mean)\n_ = sns.histplot(sampling_means)\n\n\n\n\n\nFigure 5: the sample mean is approximately of normal distribution even if the population is skewed\n\n\n\n\n\n\nIntuition\nIn simple term, CLT can be expressed as\n\n\n\n\n\n\nCentral Limit Theorem\n\n\n\nThe sum of \\(n\\) independent random variables will be approximately a normal distribution provided that each has finite variance, and that no small set of the variables contributes most of the variation.\n\n\nIntuitively, if something is affected by many independent factors, that something will likely be uniformly distributed. For example, at least 180 genes contribute to human height, and we can assume that they are largely independent: gene A gives longer neck, gene B longer torso, gene C longer bones, etc. Human height is approximately normally distributed because it is caused by the sum of the independent effects of all these genes.\nThinking in this way, we can begin to see why normal distribution can be seen everywhere in the nature. It is because natural phenomena most likely have many independent small factors."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "statistic\n\n\n\n\n\n\n\n\n\n\n\nSep 12, 2022\n\n\nCaleb\n\n\n\n\n\n\nNo matching items"
  }
]